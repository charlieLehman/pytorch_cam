# -*- coding: utf-8 -*-
"""HyperpolarClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TlVqcB7IB7yLDo7OSf2xnY1r7tbhbNJH

## Hyperpolar Classification

## Dependencies
"""

from matplotlib.image import AxesImage
class SegmentationAnimation(animation.TimedAnimation):
    def __init__(self, original, segmented, probability_frames, classes, class_choice):
        
        self.pbf = probability_frames
        self.original = original
        self.segmented = segmented
        self.classes = list(classes)
        self.classes.append('all')
        self.classes.append('attention')
        self.choice = self.classes.index(class_choice)
        
        self.fig, (self.ax1, self.ax2) = plt.subplots(1,2, figsize=(10,10), tight_layout=True)
        self.concat_im = np.concatenate([original, segmented[self.choice]], 1)
        
        self.frame_count, h,w = original.shape[0], original.shape[1], original.shape[2]
        self.emp_shape = self.concat_im[0].shape
        self.im_anim = self.ax1.imshow(np.empty(self.emp_shape))
        self.ax1.set_xticks([])
        self.ax1.set_yticks([])
        
        self.barch = self.ax2.barh(classes, np.empty_like(self.pbf[0]))
        if self.choice == 10:
            for n in range(10):
                self.barch[n].set_color(hsv_to_rgb([(n+0.5)/10,1,1]))
        elif self.choice == 11:
            for n in range(10):
                self.barch[n].set_color('blue')
        else:
            self.barch[self.choice].set_color('r')
        self.ax2.set_xlim([0,1])
        self.asp = np.diff(self.ax2.get_xlim())[0] / np.diff(self.ax2.get_ylim())[0]
        self.ax2.set_aspect(self.asp)
        self.ax2.invert_yaxis()
        self.ax2.yaxis.set_tick_params(labelsize=20)
        self.ax2.xaxis.set_tick_params(labelsize=20)
        animation.TimedAnimation.__init__(self, self.fig, interval=50, blit=True)
        
    def update_choice(self, class_choice):    
        plt.close('all')
        self.__init__(self.original, self.segmented, self.pbf, self.classes[:-2], class_choice)
        
    def _draw_frame(self, framedata):
        i = framedata
        da = self.concat_im[i]
        self.im_anim.set_data(da)
        self._drawn_artists = [self.im_anim]
        for bar, data in zip(self.barch, self.pbf[i]):
            bar.set_width(data)
        self._drawn_artists.append(self.barch)
        
    def new_frame_seq(self):
        return iter(range(self.frame_count))

    def _init_draw(self):
        self.im_anim.set_data(np.empty(self.emp_shape))
        for bar, data in zip(self.barch, self.pbf[0]):
            bar.set_width(data)

# %load_ext autoreload
# %autoreload 2
from time import time
import torch
from torch import nn
from torch.autograd import Variable
import torchvision.models as models
from torchvision.datasets import CIFAR10, MNIST, EMNIST, FashionMNIST, ImageFolder, CocoDetection
import torchvision.transforms as transforms
from torchvision.utils import save_image, make_grid
from matplotlib import pyplot as plt
from matplotlib.colors import hsv_to_rgb
from matplotlib.image import BboxImage
from matplotlib.transforms import Bbox, TransformedBbox
from scipy.stats import norm
import math
from livelossplot import PlotLosses
import numpy as np
from networks.wide_resnet import Wide_ResNet
from networks.cae import SegNet_Classifier
from datasets.voc import VOCClassSegBase as VOC
from IPython import display
import requests
from io import BytesIO
from PIL import Image
from PIL import Image, ImageSequence
from IPython.display import HTML
from skimage.transform import resize as imresize
import warnings
from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas
from matplotlib.figure import Figure
import matplotlib.animation as animation
from matplotlib import rc
import multiprocessing as mp
import gc
import matplotlib
matplotlib.rcParams['pdf.fonttype'] = 42
matplotlib.rcParams['ps.fonttype'] = 42
gc.enable()
plt.ioff()

def where(cond, x_1, x_2):
    cond = cond.float()    
    return (cond * x_1) + ((1-cond) * x_2)

"""## CIFAR-10 Data"""

transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=8),
    transforms.RandomHorizontalFlip(),
    #transforms.Resize(64),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.Resize((32, 32)),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

trainset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)
train_iter = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)

testset = CIFAR10(root='./data', train=False, download=True, transform=transform_test)
test_iter = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

"""## CINIC-10 Data"""

cinic_mean = [0.47889522, 0.47227842, 0.43047404]
cinic_std = [0.24205776, 0.23828046, 0.25874835]

transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=8),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(cinic_mean, cinic_std),
])

transform_test = transforms.Compose([
    transforms.Resize((32, 32)),
    transforms.ToTensor(),
    transforms.Normalize(cinic_mean, cinic_std),
])
cinic_trainset = ImageFolder('cinic-10/train', transform=transform_train)
cinic_valset = ImageFolder('cinic-10/valid', transform=transform_train)
cinic_train_iter = torch.utils.data.DataLoader(torch.utils.data.ConcatDataset((cinic_trainset, cinic_valset)),
                                             batch_size=64, 
                                             shuffle=True, 
                                             num_workers=4, 
                                             pin_memory=True)

cinic_testset = ImageFolder('cinic-10/test', transform=transform_test)
cinic_test_iter = torch.utils.data.DataLoader(cinic_testset, 
                                        batch_size=100, 
                                        shuffle=False, 
                                        num_workers=4, 
                                        pin_memory=True)

"""## Train"""

model = SegNet_Classifier(10)
model = nn.DataParallel(model).cuda()
num_epochs = 300
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.9, weight_decay=0.0001)
lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)

losses = []
acces = []
v_losses = []
v_acces = []
for epoch in range(num_epochs):
    epoch_loss = 0.0
    acc = 0.0
    model.train()
    for i, (x, label) in enumerate(cinic_train_iter):
        lr_scheduler.step()
        x = Variable(x).cuda()
        label = Variable(label).cuda()
        logit = model(x)
        loss = criterion(logit, label)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
        acc += (logit.argmax(-1)==label).sum()
    avg_loss = epoch_loss / (i + 1)
    losses.append(avg_loss)
    avg_acc = acc.cpu().detach().numpy() / len(cinic_trainset+cinic_valset)
    acces.append(avg_acc)
    model.eval()
    epoch_loss = 0.0
    acc = 0.0
    
    im_l = []
    for i, (x,label) in enumerate(cinic_test_iter):
        x = Variable(x).cuda()
        if i%90==0:
            im_l.append(x[0])
        label = Variable(label).cuda()
        logit = model(x)
        loss = criterion(logit, label)
        epoch_loss += loss.item()
        acc += (logit.argmax(-1)==label).sum()
    avg_loss_val = epoch_loss / (i + 1)
    v_losses.append(avg_loss_val)
    avg_acc_val = acc.cpu().detach().numpy() / len(cinic_testset)
    v_acces.append(avg_acc_val)

    x = torch.stack(im_l)
    seg_out, _ = model.module.pixelwise_classifier(x)
    conf = torch.max(nn.functional.softmax(seg_out, dim=1), dim=1)[0]
    #conf = where(conf>0.95, conf,0)
    hue = (torch.argmax(seg_out, dim=1).float() + 0.5)/10
    gs_im = x.mean(1)
    gs_mean = gs_im.mean()
    gs_min = gs_im.min()
    gs_max = torch.max((gs_im-gs_min))
    gs_im = (gs_im - gs_min)/gs_max
    hsv_im = torch.stack((hue.float(), conf.float(), gs_im.float()), -1)
    im = hsv_to_rgb(hsv_im.cpu().detach().numpy())
    ex = make_grid(torch.tensor(im).permute(0,3,1,2), normalize=True, nrow=10)
    inputs = make_grid(x, normalize=True, nrow=10).cpu().detach()
    display.clear_output(wait=True)
    plt.figure(figsize=(20,10))
    plt.imshow(np.concatenate((inputs.numpy().transpose(1,2,0),ex.numpy().transpose(1,2,0)), axis=0))
    plt.xticks(np.linspace(18,324,10), classes)
    plt.xticks(fontsize=20) 
    plt.yticks([])
    plt.title('CINIC10 Epoch:{:02d}, Train:{:.3f}, Test:{:.3f}'.format(epoch, avg_acc, avg_acc_val), fontsize=20)
    plt.savefig('anim/weak_seg_{:05d}.jpg'.format(epoch), inches='tight')
    display.display(plt.gcf())
    fig, ax = plt.subplots(1,2, figsize=(20,10))
    ax[0].set_title('Crossentropy Loss')
    ax[0].plot(losses, label='Train')
    ax[0].plot(v_losses, label='CINIC10 Test')
    ax[0].legend()
    ax[1].set_title('Accuracy')
    ax[1].plot(acces, label='Train')
    ax[1].plot(v_acces, label='CINIC10 Test')
    ax[1].legend()
    display.display(plt.gcf())
    
    torch.save(model.module.state_dict(), 'segnet_classification_50_4.pt')

model = SegNet_Classifier(10, 50, 32, 32).cuda()
model.load_state_dict(torch.load('segnet_classification_50.pt'))
transform_anim = transforms.Compose([
    transforms.Resize(32),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])
anim_data = ImageFolder('example_anims', transform=transform_anim)
anim_iter = torch.utils.data.DataLoader(anim_data, batch_size=10, shuffle=False, num_workers=1)
ii=0
for (x, l) in anim_iter:
    x = Variable(x).cuda()
    hsv_im, conf, pred, prob, hsv_im_l = model.segment(x)
    im = hsv_to_rgb(hsv_im.cpu().detach().numpy())
    x_ = x.permute(0,2,3,1).cpu().detach().numpy()
    x_ = x_ - x_.min()
    x_ = x_/x_.max()
    for n, (t,s) in enumerate(zip(x_, im)):
        plt.figure(figsize=(20,20))
        plt.imshow(np.concatenate([t,s], axis=0))
        txt = "{:.1f} % {}".format(conf[n]*100, classes[pred[n]])
        plt.text(15, 30, txt, weight='bold', horizontalalignment='center',
                 verticalalignment='center', fontsize=40)
        plt.axis('off')
        plt.savefig('anim/vids_{:05d}.jpg'.format(ii), bbox_inches='tight')
        plt.close()
        ii+=1

    #plt.figure(figsize=(20,10))
    #plt.imshow(np.concatenate((inputs.numpy().transpose(1,2,0),ex.numpy().transpose(1,2,0)), axis=0))
    #plt.xticks(np.linspace(18,324,10), ["{:.1f} % {}".format(_c*100, classes[n]) for _c, n in zip(im_lvl_conf, im_lvl_pred)])
    #plt.xticks(fontsize=15) 
    #plt.yticks([])

"""## Image Segmentation"""

#@title Live Image Segmentation Demo { vertical-output: true }
warnings.filterwarnings("ignore")
class_choice = "plane" #@param  ['all','attention', 'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
URL = "https://photos.smugmug.com/Airlines-Europe-1/Airlines-Sweden/Air-Sweden-1st-Time-Air-Sweden/i-kmjHnc6/0/134c4a08/L/Air%20Sweden%20%28Time%20Air%20Sweden%29%281st%29%20737-200%20SE-DLP%20%2891%29%28Grd%29%20MUC%20%28CVC%29%2846%29-L.jpg" #@param ["https://www.qantasnewsroom.com.au/wp-content/uploads/2016/04/737-CAR.-RESIZE-700x423.jpg", "https://c.o0bg.com/rf/image_960w/Boston/2011-2020/2015/06/02/BostonGlobe.com/Magazine/Images/shutterstock_15484942-1837.jpg", "http://www.abc.net.au/news/image/1182744-3x2-940x627.jpg", "http://www.buckmanager.com/media/images/2015/01/feral-cats-parasites-deer-hunting-disease-011215.jpg", "https://i.ytimg.com/vi/wpD4FqQZkCQ/maxresdefault.jpg", "https://www.cesarsway.com/sites/newcesarsway/files/styles/large_article_preview/public/Common-dog-behaviors-explained.jpg?itok=FSzwbBoi", "https://us.123rf.com/450wm/dikaya37/dikaya371707/dikaya37170700025/83230965-cat-and-dog-together-maine-coon-kitten-golden-retriever-look-at-right-with-sticking-out-tongues.jpg?ver=6", "https://horsej-intellectsolutio.netdna-ssl.com/cdn/farfuture/tqbnGxhFQAPyrT-barS6tBPD1NG26ASD7Z_nb0A9wLY/mtime:1527893203/files/styles/article_large/public/pictures-videos/articles/canstockphoto24623274_-_callipso88-web.jpg?itok=BdmG23rI", "http://www.amphibians.org/wp-content/uploads/2017/12/globalwildlife_57925598_Large.jpg", "https://upload.wikimedia.org/wikipedia/commons/5/55/Atelopus_zeteki1.jpg", "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSDRIc1omVVXlmZ5GLdPMI7zaVJ1T_yX6VUZf4bfG6s1ed90yhi", "https://amp.businessinsider.com/images/5637683dbd86effb5b8bb4ac-750-500.jpg", "https://www.public.navy.mil/surfor/lcc19/Photos/110218-N-9094S-186.jpg", "https://www.stripes.com/polopoly_fs/1.486660.1504838709!/image/image.jpg_gen/derivatives/landscape_900/image.jpg", "https://wallpaper-house.com/data/out/7/wallpaper2you_174528.jpg", "https://image.winudf.com/v2/image/Y29tLmFjdGlvbmdhbWVzLmpldC5wbGFuZS52cy5idWdhdHRpLmNhci5yYWNpbmcuZ2FtZXMud2FyLnBsYW5lcy5mMTguZjE2LmZsaWdodC5waWxvdC5zaW11bGF0b3Jfc2NyZWVuXzBfMTUzNzI2MTI4Ml8wMTc/screen-0.jpg?h=355&fakeurl=1&type=.jpg", "http://cl.jroo.me/z3/_/k/B/e/a.baa-bird-dog.jpg", "https://img.thrfun.com/img/072/279/mr_v_sulphurcrested_cockatoo_l.jpg", "https://www.dailydot.com/wp-content/uploads/a87/e8/cd4fc0bf531763df96769052b4b52b30.jpg"] {allow-input: true}
class_choice_l = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck', 'all', 'attention']
save_name = "AirSweden" #@param {type:"string"}
with torch.no_grad():
    response = requests.get(URL)
    img = Image.open(BytesIO(response.content))
    model = SegNet_Classifier(10).cuda()
    model.load_state_dict(torch.load('segnet_classification_50_3.pt'))
    x = Variable(transform_test(img)).cuda().unsqueeze(0)
    _hsv_im, _, _, _prob, _hsv_im_l, _attn = model.segment(x)
    prob = _prob.squeeze().cpu().numpy()
    hsv_im_l = [x.squeeze(0).cpu().numpy() for x in _hsv_im_l]
    hsv_im_l.append(_hsv_im.squeeze(0).cpu().numpy() )
    im_sel = []
    gs_im = (np.array(img)/255).mean(-1)
    attn = _attn.squeeze().cpu().numpy()
    attn -= attn.min()
    attn /= attn.max()
    attn = imresize(attn, gs_im.shape)
    attn = hsv_to_rgb(np.stack([attn, attn, gs_im], axis=-1))
    
        
    def _resize(hsv_im):
        hue = imresize(hsv_im[:,:,0], gs_im.shape)
        sat = imresize(hsv_im[:,:,1], gs_im.shape)
        im = hsv_to_rgb(np.stack([hue, sat, gs_im], axis=-1))
        return im
    
    gc.collect()
    pool = mp.Pool(processes=8)
    im_sel = pool.map(_resize, hsv_im_l)
    pool.close()
    pool.join()
    
    im_sel.append(attn)
    def _save_image(class_choice):
        fig, ax = plt.subplots(1,2, figsize=(20,10))
        if class_choice == 'all':
            the_choice = 10
        elif class_choice == 'attention':
            the_choice = 11
        else:
            the_choice = classes.index(class_choice)
        ax[0].imshow(np.concatenate([np.array(img)/255,im_sel[the_choice]], 0))
        ax[0].set_xticks([])
        ax[0].set_yticks([])
        barch = ax[1].barh(classes, prob)
        if the_choice == 10:
            for n in range(10):
                barch[n].set_color(hsv_to_rgb([(n+0.5)/10,1,1]))
        elif the_choice == 11:
            for n in range(10):
                barch[n].set_color('blue')
        else:
            barch[the_choice].set_color(hsv_to_rgb([(the_choice+0.5)/10,1,1]))
        ax[1].set_xlim([0,1])
        asp = np.diff(ax[1].get_xlim())[0] / np.diff(ax[1].get_ylim())[0]
        ax[1].set_aspect(asp)
        ax[1].invert_yaxis()
        ax[1].yaxis.set_tick_params(labelsize=20)
        ax[1].xaxis.set_tick_params(labelsize=20)
        plt.tight_layout()
        plt.savefig('demo_examples/images/{}_{}.png'.format(save_name, class_choice), bbox_inches="tight")
        
   #for _choice in class_choice_l:
   #    _save_image(_choice)
_save_image(class_choice)

#@title Live Image Saliency Demo { vertical-output: true }
class_choice = "plane"
URL = "https://horsej-intellectsolutio.netdna-ssl.com/cdn/farfuture/tqbnGxhFQAPyrT-barS6tBPD1NG26ASD7Z_nb0A9wLY/mtime:1527893203/files/styles/article_large/public/pictures-videos/articles/canstockphoto24623274_-_callipso88-web.jpg?itok=BdmG23rI" #@param ["https://www.qantasnewsroom.com.au/wp-content/uploads/2016/04/737-CAR.-RESIZE-700x423.jpg", "https://c.o0bg.com/rf/image_960w/Boston/2011-2020/2015/06/02/BostonGlobe.com/Magazine/Images/shutterstock_15484942-1837.jpg", "http://www.abc.net.au/news/image/1182744-3x2-940x627.jpg", "http://www.buckmanager.com/media/images/2015/01/feral-cats-parasites-deer-hunting-disease-011215.jpg", "https://i.ytimg.com/vi/wpD4FqQZkCQ/maxresdefault.jpg", "https://www.cesarsway.com/sites/newcesarsway/files/styles/large_article_preview/public/Common-dog-behaviors-explained.jpg?itok=FSzwbBoi", "https://us.123rf.com/450wm/dikaya37/dikaya371707/dikaya37170700025/83230965-cat-and-dog-together-maine-coon-kitten-golden-retriever-look-at-right-with-sticking-out-tongues.jpg?ver=6", "https://horsej-intellectsolutio.netdna-ssl.com/cdn/farfuture/tqbnGxhFQAPyrT-barS6tBPD1NG26ASD7Z_nb0A9wLY/mtime:1527893203/files/styles/article_large/public/pictures-videos/articles/canstockphoto24623274_-_callipso88-web.jpg?itok=BdmG23rI", "http://www.amphibians.org/wp-content/uploads/2017/12/globalwildlife_57925598_Large.jpg", "https://upload.wikimedia.org/wikipedia/commons/5/55/Atelopus_zeteki1.jpg", "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSDRIc1omVVXlmZ5GLdPMI7zaVJ1T_yX6VUZf4bfG6s1ed90yhi", "https://amp.businessinsider.com/images/5637683dbd86effb5b8bb4ac-750-500.jpg", "https://www.public.navy.mil/surfor/lcc19/Photos/110218-N-9094S-186.jpg", "https://www.stripes.com/polopoly_fs/1.486660.1504838709!/image/image.jpg_gen/derivatives/landscape_900/image.jpg", "https://wallpaper-house.com/data/out/7/wallpaper2you_174528.jpg", "https://image.winudf.com/v2/image/Y29tLmFjdGlvbmdhbWVzLmpldC5wbGFuZS52cy5idWdhdHRpLmNhci5yYWNpbmcuZ2FtZXMud2FyLnBsYW5lcy5mMTguZjE2LmZsaWdodC5waWxvdC5zaW11bGF0b3Jfc2NyZWVuXzBfMTUzNzI2MTI4Ml8wMTc/screen-0.jpg?h=355&fakeurl=1&type=.jpg", "http://cl.jroo.me/z3/_/k/B/e/a.baa-bird-dog.jpg", "https://img.thrfun.com/img/072/279/mr_v_sulphurcrested_cockatoo_l.jpg", "https://www.dailydot.com/wp-content/uploads/a87/e8/cd4fc0bf531763df96769052b4b52b30.jpg"] {allow-input: true}
class_choice_l = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck', 'all', 'attention']
with torch.no_grad():
    response = requests.get(URL)
    img = Image.open(BytesIO(response.content))
    model = SegNet_Classifier(10).cuda()
    model.load_state_dict(torch.load('segnet_classification_50_3.pt'))
    x = Variable(transform_test(img)).cuda().unsqueeze(0)
    _hsv_im, _, _, _prob, _hsv_im_l, _attn = model.segment(x)
    prob = _prob.squeeze().cpu().numpy()
    
    hsv_im_l = [x.squeeze(0).cpu().numpy() for x in _hsv_im_l]
    hsv_im_l.append(_hsv_im.squeeze(0).cpu().numpy() )
    im_sel = []
    gs_im = (np.array(img)/255).mean(-1)
    attn = _attn.squeeze(0).cpu().numpy()
    attn /= attn.max()
    attn = attn.squeeze()
    fig, ax = plt.subplots(1,2, dpi=250)
    ax[1].imshow(gs_im*imresize(attn, gs_im.shape), cmap='gray')
    ax[1].axis('off')
    ax[0].imshow(img)
    ax[0].axis('off')

"""## GIF Segmentation"""

#@title Live GIF Segmentation Demo { vertical-output: true }
warnings.filterwarnings("ignore")
class_choice = "dog" #@param  ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck', 'all', 'attention']
class_choice_l = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck', 'all', 'attention']
URL = "https://media.giphy.com/media/YIW0KqAQShjCE/giphy.gif" #@param [] {allow-input: true}
save_name = "DogAnnoyed" #@param {type:"string"}
response = requests.get(URL)
img = Image.open(BytesIO(response.content))
frames_np = np.stack([np.array(frame.convert('RGB'))/255 for frame in ImageSequence.Iterator(img)])
frames_ = torch.stack([transform_test(frame.convert('RGB')) for frame in ImageSequence.Iterator(img)])
with torch.no_grad():
    model = SegNet_Classifier(10).cuda()
    model.load_state_dict(torch.load('segnet_classification_50_4.pt'))
    x = Variable(frames_).cuda()
    _hsv_im, _conf, _pred, _prob, _hsv_im_l, _attn = model.segment(x)
    
conf = _conf.cpu().numpy()
pred = _pred.cpu().numpy()
prob = _prob.cpu().numpy()

hsv_im_l = [x.cpu().numpy() for x in _hsv_im_l]
hsv_im_l.append(_hsv_im.squeeze(0).cpu().numpy() )
im_sel = []
frames = frames_np
frames -= frames.min()
frames /= frames.max()
gs_im = frames.mean(-1)
attn = _attn.squeeze().cpu().numpy()
attn -= attn.min()
attn /= attn.max()
attn = imresize(attn, gs_im.shape)
attn = hsv_to_rgb(np.stack([attn, attn, gs_im], axis=-1))
def _resize(hsv_im):
    im = hsv_to_rgb(np.stack([imresize(hsv_im[:,:,:,0], gs_im.shape), 
                              imresize(hsv_im[:,:,:,1], gs_im.shape), 
                              gs_im], axis=-1))
    return im
        
gc.collect()
pool = mp.Pool(processes=8)
im_sel = pool.map(_resize, hsv_im_l)
pool.close()
pool.join()
im_sel.append(attn)

# Set up formatting for the movie files
Writer = animation.writers['ffmpeg']
writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)

def _save_anim(_choice):
    SegmentationAnimation(frames, np.array(im_sel), prob, classes, _choice).save('demo_examples/videos/{}_{}.mp4'.format(save_name, _choice), writer=writer)


   #for _choice in class_choice_l:
   #    _save_anim(_choice)
    
# Multiprocessing causes bugs in tick alignment?!?!
#pool = mp.Pool(processes=6)
#pool.map(_save_anim, class_choice_l)
#pool.close()
#pool.join()

ani = SegmentationAnimation(frames, np.array(im_sel), prob, classes, class_choice)
HTML(ani.to_html5_video())
ani._repr_html_() is None
rc('animation', html='html5')
ani

"""## CIFAR Test"""

import PIL
with torch.no_grad():
    model = SegNet_Classifier(10).cuda()
    model.load_state_dict(torch.load('segnet_classification_50_4.pt'))
    epoch_loss = 0.0
    acc = 0.0
    im_l = []
    corr_l = []
    for i, (x,label) in enumerate(test_iter):
        x = Variable(x).cuda()
        label = Variable(label).cuda()
        logit = model(x)
        _hsv_im, _, _, _prob, _hsv_im_l, _attn = model.segment(x)
        im_l.append(_hsv_im)
        
        loss = criterion(logit, label)
        epoch_loss += loss.item()
        corr = (logit.argmax(-1)==label)
        corr_l.append(corr.cpu().numpy())
        acc += corr.sum()
    avg_loss_val = epoch_loss / (i + 1)
    im_l = torch.cat(im_l, dim=0).cpu().numpy()
    l_l = testset.test_labels
    choices = np.argwhere(np.array(l_l)==4)
    ims_ = torch.tensor(im_l[choices][:5].squeeze())
    ex = make_grid(ims_.permute(0,3,1,2), normalize=True, nrow=5).cpu().numpy()
    plt.figure(figsize=(20,20))
    plt.imshow(hsv_to_rgb(ex.transpose(1,2,0)))
    
    
        
   #def _resize(hsv_im):
   #    hue = imresize(hsv_im[:,:,0], gs_im.shape)
   #    sat = imresize(hsv_im[:,:,1], gs_im.shape)
   #    im = hsv_to_rgb(np.stack([hue, sat, gs_im], axis=-1))
   #    return im
   #
   #inputs = make_grid(x, normalize=True, nrow=10).cpu()
   #display.clear_output(wait=True)
   #plt.figure(figsize=(20,10))
   #plt.imshow(np.concatenate((inputs.numpy().transpose(1,2,0),hsv_to_rgb(ex.numpy().transpose(1,2,0))), axis=0))
   #plt.xticks(np.linspace(18,324,10), classes)
   #plt.xticks(fontsize=20) 
   #plt.yticks([])